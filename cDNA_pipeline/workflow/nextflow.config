// CONFIGURATION FILE


// Pipeline parameter default values, can be modified by user when calling pipeline on command line (e.g. --data_fq sample_1.fastq) ##
params.ont_reads_fq = 'None'
params.ont_reads_txt = 'None'
params.ref = 'None'
params.housekeeping = 'None'
params.annotation = 'None'
params.out_dir = "output_directory/"
params.is_chm13 = "False"
params.fast5_dir = 'None'
params.basecall_id = "None"
params.basecall_config = "None"
params.ercc = "None"
params.cdna_kit = "None"
params.multiqc_config = "None"
params.is_discovery = "None"
params.NDR = "auto"
params.bambu_track_reads = "False"

process { 


    withLabel: tiny {
        executor='slurm'
        clusterOptions='--partition normal --time 00:15:00 --account cca_mteb223_uksr --nodes 1 --ntasks 4 --mem 20G'
    }



    withLabel: small {
        executor='slurm'
        clusterOptions='--partition normal --time 1:00:00 --account cca_mteb223_uksr --nodes 1 --ntasks 8 --mem 40G'
    }



    // Define job scheduler parameters for jobs that require lots of computation/memory ##

    withLabel: medium_small {
        
        executor='slurm'
        clusterOptions='--partition normal --time 23:00:00 --account cca_mteb223_uksr --nodes 1 --ntasks 12 --mem 48G'
        }


    // Define job scheduler parameters for jobs that require normal ammounts of computation/memory ##

    withLabel: medium {
        executor='slurm'
        clusterOptions='--partition normal --time 23:00:00 --account cca_mteb223_uksr --nodes 1 --ntasks 16 --mem 64G'
        }

    withLabel: medium_large {
        executor='slurm'
        clusterOptions='--partition normal --time 23:00:00 --account cca_mteb223_uksr --nodes 1 --ntasks 20 --mem 80G'
    }

    withLabel: large {
        executor='slurm'
        clusterOptions='--partition normal --time 23:00:00 --account cca_mteb223_uksr --nodes 1 --ntasks 50 --mem 200G'
    }

    withLabel: huge {
        executor='slurm'
        clusterOptions='--partition normal --time 23:00:00 --account cca_mteb223_uksr --nodes 1 --ntasks 8 --mem 500G'
    }

    withLabel: local {
        executor='local'
    }

    withLabel: gpu {
        clusterOptions='--partition P4V12_SKY32M192_L --time 00:15:00 --account gol_mteb223_uksr --gres=gpu:1 --mem 16G'
    }

    // Define the singularity containers for each process

    // Nanopore
    withName: "(GFFCOMPARE|MAKE_INDEX_cDNA|MINIMAP2_cDNA|MINIMAP2_QC|PYCHOPPER|STRINGTIE_ONT_cDNA|MAKE_FAI|MAKE_TRANSCRIPTOME)" {
        container = "file://../../singularity_container/nanopore.sif"
    }

    // Quality Control
    withName: "(MULTIQC_GRCh38|MULTIQC_CHM13|RSEQC|PYCOQC|DECOMPRESS|TRIM_GALORE|CHM13_GTF_ERCC|CHM13_GTF)" {
        container = "file://../../singularity_container/quality_control.sif"
    }

    // Basecalling
    withName: "(BASECALL|GATHER_BASECALL)" {
        container = "file://../../singularity_container/guppy.sif"
        singularity.runOptions='--nv'
    }

    // Bambu
    withName: "(BAMBU_PREP_DISCOVERY|BAMBU_PREP_QUANT|BAMBU_DISCOVERY|BAMBU_QUANT)" {
        container = "file://../../singularity_container/bambu.sif"
    }

}



// Define executor type and maximum queue size for jobs at once ##

executor {

    name="slurm"
    queueSize = 50
}

// Point to singularity image with the tools necessary to run the pipeline

singularity {
    
    enabled = true
}


{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68e81ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import csv\n",
    "\n",
    "\n",
    "## Display all rows of pandas dataframes\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "079e7e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "function name: fix_column_names\n",
    "\n",
    "purpose: Fixing the column names, making them smaller, informative, and consistent\n",
    "\n",
    "input: The raw counts dataframe for either genes or transcripts \n",
    "\n",
    "output: Same dataframe with improved column names\n",
    "'''\n",
    "\n",
    "def fix_column_names(df, is_gene=False):\n",
    "    \n",
    "    ## Check if this is a gene counts object\n",
    "    if is_gene:\n",
    "        \n",
    "        ## Get count column names and create list of new column names\n",
    "        count_columns = df.columns.tolist()\n",
    "        list_new_names = [\"gene_id\"]\n",
    "        \n",
    "        ## gene_id comes in as index for gene counts data, make it into the first column instead\n",
    "        df[\"gene_id\"] = df.index\n",
    "        cols = list(df.columns)\n",
    "        cols = [cols[-1]] + cols[:-1]\n",
    "        df = df[cols]\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    ## If it is a transcript dataset\n",
    "    else:\n",
    "        ## Set count columns and create list of new names\n",
    "        count_columns = df.columns[2:].tolist()\n",
    "        list_new_names = [ \"transcript_id\", \"gene_id\"]\n",
    "    \n",
    "    ## Fix names one by one and add to list of new names\n",
    "    for col in count_columns:\n",
    "        col = col.split(\"_mapped\")[0] + \"_counts\"\n",
    "        list_new_names.append(col)\n",
    "    \n",
    "    ## Rename columns\n",
    "    df.columns = list_new_names\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bd065cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "function name: parse_df_columns\n",
    "\n",
    "purpose: parsing the last aggregate column of the gtf/gff3 into useful columns and cleaning non-relevant columns\n",
    "\n",
    "input: dataframe containining \"raw\" gtf/gff\n",
    "\n",
    "output: dataframe containing gtf with useful columns [\"gene_id\", \"transcript_id\", etc...]\n",
    "'''\n",
    "\n",
    "def parse_df_columns(df, is_ref=True, is_transcript=False, is_exon=False):\n",
    "\n",
    "    if is_ref:\n",
    "\n",
    "        ## Get gene ids\n",
    "        df[\"gene_id\"] = df[\"other\"].str.split('\";', expand=True)[0].str.extract(\"([^ \\\"]*$)\", expand=True)\n",
    "        \n",
    "        ## Get gene names\n",
    "        df[\"gene_name\"] = df[\"other\"].str.split(\"gene_name \\\"\", expand=True)[1].str.split('\\\";', expand=True)[0]\n",
    "        \n",
    "        ## Get get transcript biotype\n",
    "        df[\"gene_biotype\"] = df[\"other\"].str.split('gene_biotype \"', expand=True)[1].str.split('\"', expand=True)[0]\n",
    "        \n",
    "        if is_transcript:\n",
    "            df[\"transcript_id\"] = df[\"other\"].str.split('transcript_id \"', expand=True)[1].str.split('\"', expand=True)[0]\n",
    "            df[\"transcript_biotype\"] = df[\"other\"].str.split('transcript_biotype \"', expand=True)[1].str.split('\"', expand=True)[0]\n",
    "        \n",
    "        if is_exon:\n",
    "            df[\"transcript_id\"] = df[\"other\"].str.split('transcript_id \"', expand=True)[1].str.split('\"', expand=True)[0]\n",
    "            df[\"transcript_biotype\"] = df[\"other\"].str.split('transcript_biotype \"', expand=True)[1].str.split('\"', expand=True)[0]\n",
    "            df[\"exon_number\"] = df[\"other\"].str.split('exon_number \"', expand=True)[1].str.split('\"', expand=True)[0]\n",
    "\n",
    "            \n",
    "        ## Drop \"other\" column\n",
    "        df.drop(columns=[\"other\", \"dot_1\", \"dot_2\"], inplace=True)\n",
    "        \n",
    "\n",
    "    else:\n",
    "\n",
    "        ## Get gene ids\n",
    "        df[\"gene_id\"] = df[\"other\"].str.split('\";', expand=True)[0].str.extract(\"([^ \\\"]*$)\", expand=True)\n",
    "\n",
    "        ## Get transcript ids\n",
    "        df[\"transcript_id\"] = df[\"other\"].str.split('transcript_id \"', expand=True)[1].str.split('\"', expand=True)[0]\n",
    "\n",
    "        ## Get exon number\n",
    "        df[\"exon_number\"] = df[\"other\"].str.split('exon_number \"', expand=True)[1].str.split('\"', expand=True)[0]\n",
    "\n",
    "        ## Drop \"other\" column\n",
    "        df.drop(columns=[\"other\", \"dot_1\", \"dot_2\"], inplace=True)\n",
    "\n",
    "    for col in df.columns:\n",
    "        df.loc[df[col].isnull(), col] = np.NaN\n",
    "        \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a04987b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "function name: calculate_cpm\n",
    "\n",
    "purpose: Calculate CPM for the each sample given\n",
    "\n",
    "input: Counts dataset\n",
    "\n",
    "output: Counts dataset with CPM columns as well\n",
    "'''\n",
    "\n",
    "def calculate_cpm(df, is_gene=False):\n",
    "\n",
    "    ## Set count columns if dataframe is gene counts\n",
    "    if is_gene:\n",
    "        count_columns = df.columns[1:].tolist()\n",
    "    \n",
    "    ## Set count columns if dataframe is transcript counts\n",
    "    else:\n",
    "        count_columns = df.columns[2:].tolist()\n",
    "\n",
    "    ## Loop through counts columns to calculate CPM and add to the dataframe\n",
    "    for col in count_columns:\n",
    "        \n",
    "        df[col] = df[col]\n",
    "        cpm_name = col.replace(\"_counts\", \"_CPM\")\n",
    "        df[cpm_name] = ((df[col]/(df[col].sum())) * 1000000)\n",
    "    \n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b1633af",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../../data/bernardo/raw/merged_aged_stringent/bambu_discovery/extended_annotations.gtf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m original_ref \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../../../references/bernardo/Homo_sapiens.GRCh38.107_ERCC.gtf\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[1;32m      5\u001b[0m                        names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdot_1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrand\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdot_2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mother\u001b[39m\u001b[38;5;124m\"\u001b[39m], comment\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m## Bambu reference with novel and annotated transcripts\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m bambu_ref \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../../../../data/bernardo/raw/merged_aged_stringent/bambu_discovery/extended_annotations.gtf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msource\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstart\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdot_1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrand\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdot_2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mother\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m## New Transcript Events\u001b[39;00m\n\u001b[1;32m     14\u001b[0m df_events \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../../../data/bernardo/processed/03.gene_and_transcripts_descriptive_stats/merged_aged_stringent_ndr.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/conda/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/conda/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/conda/lib/python3.10/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../../data/bernardo/raw/merged_aged_stringent/bambu_discovery/extended_annotations.gtf'"
     ]
    }
   ],
   "source": [
    "## Import Data\n",
    "\n",
    "## Open original reference\n",
    "original_ref = pd.read_csv(\"../../../../references/bernardo/Homo_sapiens.GRCh38.107_ERCC.gtf\", header=None, delimiter=\"\\t\", low_memory=False, \n",
    "                       names=[\"chr\", \"source\", \"type\", \"start\", \"end\", \"dot_1\", \"strand\", \"dot_2\", \"other\"], comment=\"#\")\n",
    "\n",
    "\n",
    "\n",
    "## Bambu reference with novel and annotated transcripts\n",
    "bambu_ref = pd.read_csv(\"../../../../data/bernardo/raw/merged_aged_stringent/bambu_discovery/extended_annotations.gtf\", header=None, delimiter=\"\\t\",\n",
    "                        low_memory=False, names=[\"chr\", \"source\", \"type\", \"start\", \"end\", \"dot_1\", \"strand\", \"dot_2\", \"other\"])\n",
    "\n",
    "## New Transcript Events\n",
    "df_events = pd.read_csv(\"../../../../data/bernardo/processed/03.gene_and_transcripts_descriptive_stats/merged_aged_stringent_ndr.tsv\", sep=\"\\t\")\n",
    "\n",
    "\n",
    "## Bambu counts matrix\n",
    "df = pd.read_csv(\"../../../../data/bernardo/raw/merged_aged_stringent/bambu_discovery/counts_transcript.txt\", \n",
    "                           delimiter=\"\\t\", low_memory=False, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9127e61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get all transcript IDs from ENSEMBL 94 GTF\n",
    "\n",
    "df_ensembl_94 = pd.read_csv(\"../../../../references/bernardo/Homo_sapiens.GRCh38.94.gtf\", header=None, delimiter=\"\\t\",\n",
    "                        low_memory=False, names=[\"chr\", \"source\", \"type\", \"start\", \"end\", \"dot_1\", \"strand\", \"dot_2\", \"other\"])\n",
    "\n",
    "df_ensembl_94 = df_ensembl_94.loc[df_ensembl_94[\"type\"] == \"transcript\"].copy()\n",
    "\n",
    "df_ensembl_94 = parse_df_columns(df_ensembl_94, is_ref=True, is_transcript=True)\n",
    "\n",
    "ensembl_94_transcript_ids = df_ensembl_94[\"transcript_id\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883157ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parse through original reference to get all needed info\n",
    "\n",
    "orig_ref_transcripts = original_ref.loc[original_ref[\"type\"]==\"transcript\"].copy()\n",
    "orig_ref_exons = original_ref.loc[((original_ref[\"type\"]==\"exon\") | (original_ref[\"type\"]==\"CDS\"))].copy()\n",
    "\n",
    "orig_ref_transcripts = parse_df_columns(orig_ref_transcripts, is_ref=True, is_transcript=True)\n",
    "orig_ref_exons = parse_df_columns(orig_ref_exons, is_ref=True, is_exon=True)\n",
    "\n",
    "orig_ref_transcripts.drop(columns=[\"source\"], inplace=True)\n",
    "orig_ref_transcripts[\"exon_number\"] = np.nan\n",
    "\n",
    "orig_ref_exons.drop(columns=[\"source\"], inplace=True)\n",
    "\n",
    "orig_ref = pd.concat([orig_ref_transcripts, orig_ref_exons]).sort_values(by=[\"chr\", \"start\", \"type\", \"end\"], \n",
    "                                                                                    ascending=[True, True, False, True])\n",
    "\n",
    "\n",
    "orig_ref.loc[orig_ref[\"transcript_id\"].isin(ensembl_94_transcript_ids), \"annotation_status\"] = \"Annotated in ENSEMBL 94 (2019)\"\n",
    "orig_ref.loc[~orig_ref[\"transcript_id\"].isin(ensembl_94_transcript_ids), \"annotation_status\"] = \"NOT Annotated in ENSEMBL 94 (2019)\"\n",
    "\n",
    "orig_ref[\"discovery_category\"] = \"Annotated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b453522",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleanup df_events\n",
    "df_events = df_events[[\"TXNAME\", \"newTxClass\"]]\n",
    "\n",
    "## Cleanup net transcript classes\n",
    "df_events.loc[df_events[\"newTxClass\"] == \"newGene-spliced\", \"newTxClass\"] = \"New Gene Body\"\n",
    "\n",
    "df_events.loc[df_events[\"newTxClass\"] == \"newWithin\", \"newTxClass\"] = \"New Combo of Known Junctions/Exons\"\n",
    "df_events.loc[df_events[\"newTxClass\"] == \"newFirstJunctionnewFirstExon\", \"newTxClass\"] = \"New First Exon\"\n",
    "df_events.loc[df_events[\"newTxClass\"] == \"newLastJunctionnewLastExon\", \"newTxClass\"] = \"New Last Exon\"\n",
    "df_events.loc[df_events[\"newTxClass\"] == \"newFirstJunction\", \"newTxClass\"] = \"New Last Exon\"\n",
    "df_events.loc[df_events[\"newTxClass\"] == \"allNew\", \"newTxClass\"] = \"All New\"\n",
    "\n",
    "df_events.loc[df_events[\"newTxClass\"] == \"newFirstJunction\", \"newTxClass\"] = \"New Junction\"\n",
    "df_events.loc[df_events[\"newTxClass\"] == \"newLastJunctionnewJunction\", \"newTxClass\"] = \"New Junction\"\n",
    "df_events.loc[df_events[\"newTxClass\"] == \"newFirstJunctionnewJunction\", \"newTxClass\"] = \"New Junction\"\n",
    "df_events.loc[df_events[\"newTxClass\"] == \"newLastJunction\", \"newTxClass\"] = \"New Junction\"\n",
    "df_events.loc[df_events[\"newTxClass\"] == \"newJunction\", \"newTxClass\"] = \"New Junction\"\n",
    "\n",
    "df_events.loc[df_events[\"newTxClass\"] == \"newLastJunctionnewJunctionnewLastExon\", \"newTxClass\"] = \"New Last Exon & New Junction\"\n",
    "df_events.loc[df_events[\"newTxClass\"] == \"newFirstJunctionnewJunctionnewFirstExon\", \"newTxClass\"] = \"New First Exon & New Junction\"\n",
    "df_events.loc[df_events[\"newTxClass\"] == \"newLastJunctionnewFirstJunctionnewJunctionnewFirstExonnewLastExon\", \"newTxClass\"] = \"New First & Last Exon & New Junction\"\n",
    "df_events.loc[df_events[\"newTxClass\"] == \"newLastJunctionnewFirstJunctionnewJunctionnewFirstExon\", \"newTxClass\"] = \"New First Exon & New Junction\"\n",
    "\n",
    "## Change column names\n",
    "df_events.columns = [\"transcript_id\", \"discovery_category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303b9d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parse through bambu reference\n",
    "bambu_ref = parse_df_columns(bambu_ref, is_ref=False)\n",
    "\n",
    "## Only keep novel genes and Transcripts\n",
    "bambu_ref = bambu_ref.loc[bambu_ref[\"transcript_id\"].str.startswith(\"tx.\")]\n",
    "\n",
    "## Create discovery category column\n",
    "bambu_ref[\"annotation_status\"] = \"unnanotated (newly discovered)\"\n",
    "bambu_ref[\"transcript_biotype\"] = \"unnanotated (newly discovered)\"\n",
    "bambu_ref = bambu_ref.merge(df_events, on=\"transcript_id\", how=\"inner\").drop_duplicates()\n",
    "\n",
    "## Get gene names\n",
    "gene_names = original_ref.loc[original_ref[\"type\"]==\"gene\"].copy()\n",
    "gene_names = parse_df_columns(gene_names, is_ref=True)\n",
    "gene_names = gene_names[[\"gene_id\", \"gene_name\"]]\n",
    "\n",
    "## Add gene_name\n",
    "bambu_ref = bambu_ref.merge(gene_names, on=\"gene_id\", how=\"left\").drop_duplicates().drop(columns=\"source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbf3d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Final Reference\n",
    "final_ref = pd.concat([orig_ref, bambu_ref]).sort_values(by=[\"chr\", \"start\", \"type\", \"end\"], \n",
    "                                                                                    ascending=[True, True, False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31670b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fix transcript biotypes\n",
    "\n",
    "## Create CDS_not_defined classification\n",
    "final_ref.loc[((final_ref[\"gene_biotype\"] == \"protein_coding\") & (final_ref[\"transcript_biotype\"] == \"processed_transcript\")), \"transcript_biotype\"] = \"cds_not_defined\"\n",
    "\n",
    "## Create other classification\n",
    "final_ref.loc[~final_ref[\"transcript_biotype\"].isin([\"protein_coding\", \"nonsense_mediated_decay\", \"lncRNA\", \"retained_intron\", \"cds_not_defined\", \"processed_transcript\", \"unnanotated (newly discovered\"]), \"transcript_biotype\"] = \"other\"\n",
    "\n",
    "## Drop gene_biotype column\n",
    "final_ref.drop(columns=\"gene_biotype\", inplace=True)\n",
    "\n",
    "## Only keep CDS regions for protein_coding transcripts\n",
    "final_ref_cds = final_ref.loc[final_ref[\"type\"] == \"CDS\"].copy()\n",
    "final_ref_cds_prot = final_ref_cds.loc[final_ref_cds[\"transcript_biotype\"] == \"protein_coding\"].copy()\n",
    "final_ref_2 = final_ref.loc[final_ref[\"type\"] != \"CDS\"].copy()\n",
    "final_final_ref = pd.concat([final_ref_2, final_ref_cds_prot]).sort_values(by=[\"chr\", \"start\", \"type\", \"end\"], \n",
    "                                                                                    ascending=[True, True, False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccab976f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save files\n",
    "final_final_ref.to_csv(\"../../../../data/bernardo/processed/99.other/create_annotations/annotation_for_maddy_r_shiny_app/annotation_r_shiny.tsv\", sep=\"\\t\", \n",
    "                index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe36a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fix expression matrix\n",
    "df = fix_column_names(df, is_gene=False)\n",
    "df = calculate_cpm(df, is_gene=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a98110",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save expression Matrix\n",
    "df.to_csv(\"../../../../data/bernardo/processed/99.other/create_annotations/annotation_for_maddy_r_shiny_app/expression_matrix_r_shiny.tsv\", sep=\"\\t\",\n",
    "         index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097aee95",
   "metadata": {},
   "source": [
    "# Create gene level file with total counts and CPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d21e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import gene level bambu counts matrix\n",
    "df_gene =  pd.read_csv(\"../../../../data/bernardo/raw/merged_aged_stringent/bambu_discovery/counts_gene.txt\", \n",
    "                           delimiter=\"\\t\", low_memory=False, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3198d745",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate total_counts, drop other counts columns\n",
    "df_gene = fix_column_names(df_gene, is_gene=True)\n",
    "counts_columns = df_gene.columns[1:].to_list()\n",
    "df_gene[\"total_counts\"] = df_gene[counts_columns].sum(axis=1)\n",
    "df_gene.drop(columns=counts_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d0ccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate CPM and give it proper name\n",
    "df_gene = calculate_cpm(df_gene, is_gene=True)\n",
    "df_gene[\"average_CPM\"] = df_gene[\"total_CPM\"].copy()\n",
    "df_gene.drop(columns=\"total_CPM\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15aa914",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save file\n",
    "df_gene.to_csv(\"../../../../data/bernardo/processed/99.other/create_annotations/annotation_for_maddy_r_shiny_app/expression_matrix_r_shiny_GENE.tsv\",\n",
    "              sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfeef9cd",
   "metadata": {},
   "source": [
    "# Create annotations to check for protein sequences for Maddy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f0d9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Open original reference\n",
    "original_ref = pd.read_csv(\"../../../../references/bernardo/Homo_sapiens.GRCh38.107_ERCC.gtf\", header=None, delimiter=\"\\t\", low_memory=False, \n",
    "                       names=[\"chr\", \"source\", \"type\", \"start\", \"end\", \"dot_1\", \"strand\", \"dot_2\", \"other\"], comment=\"#\")\n",
    "\n",
    "\n",
    "\n",
    "## Bambu reference with novel and annotated transcripts\n",
    "bambu_ref = pd.read_csv(\"../../../../data/bernardo/raw/merged_aged_stringent/bambu_discovery/extended_annotations.gtf\", header=None, delimiter=\"\\t\",\n",
    "                        low_memory=False, names=[\"chr\", \"source\", \"type\", \"start\", \"end\", \"dot_1\", \"strand\", \"dot_2\", \"other\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13189047",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter only new transcripts\n",
    "bambu_ref = bambu_ref.loc[bambu_ref[\"other\"].str.contains('transcript_id \"tx.')]\n",
    "\n",
    "\n",
    "## Filter only protein coding transcripts\n",
    "original_ref = original_ref.loc[original_ref[\"other\"].str.contains('transcript_biotype \"protein_coding\";')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5a2cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save files\n",
    "bambu_ref.to_csv(\"../../../../data/bernardo/processed/99.other/create_annotations/annotation_for_maddy_r_shiny_app/only_new_transcripts_no_filter.gtf\", sep=\"\\t\", \n",
    "                index=False, header=False, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "original_ref.to_csv(\"../../../../data/bernardo/processed/99.other/create_annotations/annotation_for_maddy_r_shiny_app/only_protein_coding_transcripts_no_filter.gtf\", sep=\"\\t\", \n",
    "                index=False, header=False, quoting=csv.QUOTE_NONE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
